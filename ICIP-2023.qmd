---
title: "ICIP-2023"
---

## Privacy attacks in computer vision

**Duration**: Half-Day Workshop

**Presenters**: Gianmarco Aversano ([Euranova](http://euranova.eu/), gianmarco.aversano@euranova.eu), Sabri Shkiri ([Euranova](http://euranova.eu/), sabri.shkiri@euranova.eu), Yixi Xu (Microsoft, yixi.xu@microsoft.com)

**Prensenters' biography**:
* *Gianmarco Aversano* is a Data Scientist in Euranova's Research Department. During his doctorate, he worked at CentraleSupélec Paris, University of Utah, and Université Libre de Bruxelles (ULB). His professional experience focuses on machine learning with a background in data engineering and MLOps, with both the knowledge gained from research and the knowledge acquired in the industrial field. Currently, Gianmarco Aversano contributes to the BISHOP program (https://research.euranova.eu/bishop/) which focuses on graph data generation and privacy risk assessment and mitigation. Besides, Euranova has experience in computer vision [see this publication where I personally contributed and also here] and is also contributing to this field in an upcoming scientific submission in the domain of privacy assessment for generative models of graph data.
* *Yixi Xu* is a senior applied research scientist at Microsoft AI for Good Lab mainly focusing on AI for health including its privacy risk estimation. Yixi Xu has a very relevant scientific publication in the domain of privacy assessment for generative models: [MACE](https://arxiv.org/pdf/2009.05683.pdf). In their paper, the authors run experiments on dataset from the computer vision community.
* *Sabri Shkiri* leads the Euranova R&D department and managse internal research projects, back office R&D requests, technological watch for customers, technical assessments and innovation forum within Euranova. Sabri Shkiri has been: program Committee member of the IEEE Big Data conference 2023, 2022, & 2021 and of the Stream Reasoning workshop 2021; program committee member of the EAI Mobicase 2022 conference; program co-chair of the Industrial track of ECML PKDD 2021 and DEBS 2021; PC member of DEBS industrial track 2023& 2022; program co-chair of the Workshop on real-time & stream analytics in Big Data, colocated with the IEEE Big Data since 2016; keynote speaker at WETICE 2021, speaker at the industrial keynote of ICML 2020, ICSOC 2019, Worldwide AI Show Dubai 2019, myData 2017, ICT Spring 2017, HPS Powercard 2015, Big Data Technologies China 2014, Huawei Strategy & Technology Workshop 2012 & 2014, EBISS 2012, ECLIPSE CON 2009 & 2013, IBM GSE 2012, FOSDEM 2010; member of the Jury for FRIA PhD Scholarship of the FNRS from 2021 to 2023.

### Description

Machine Learning models are vulnerable to privacy attacks, and computer vision (CV) models are no exception to this. This means that simply publishing a pretrained image classifier or generator opens the door to attacks such as Membership Inference (MI), which aims to predict if a target image was part of the training set. Understanding why privacy attacks are possible and how to prevent them is crucial towards the objective of building Responsible AI tools.

In this workshop, we will cover the subject of privacy attacks in CV by introducing:

1. Privacy in Machine Learning: what is it?

2. The main Privacy Attacks against Machine Learning models

3. How to defend Machine Learning models from Privacy Attacks

4. Privacy vs Utility: optimizing the privacy-utility trade-off

5. Privacy Auditing: how to estimate any privacy risk in advance?

6. Invited speakers

7. Hands-on sessions

We will also show practical examples using open-source libraries. Finally, we will also put extra focus on the synthetic image generators, which have been gaining momentum recently, and which we fear may be used wrongly.

We believe this workshop can be relevant for the following reasons:

* make the CV community aware of the privacy-utility trade-off that also exists in the image domain;
* provide CV practitioners with the theoretical and practical tools that are needed to estimate the privacy-utility trade-off in their use-case;
* given the increasing popularity of image generative models, this workshop can provide model-agnostic fundamentals on how to assess the privacy leakage of such models and the images they generate in a robust manner.

This workshop will be composed of keynote sessions on the theoretical background, keynote sessions for invited speakers whose papers have been accepted by this workshop, and one session on practical use-cases. The workshop will be held by Euranova's R&D department.

Euranova is a Belgian company who actively contributes to scientific research via their R&D department (https://research.euranova.eu/), which counts several publications in many AI-related domains. Euranova has already participated in international conferences. Besides, the organizers also have experience from their Ph.D. days.